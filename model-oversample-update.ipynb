{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "pd.options.display.max_columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('https://raw.githubusercontent.com/mkleinbort/Kaggle-COMPAS/main/train/X_train.csv', index_col='id')\n",
    "y_train = pd.read_csv('https://raw.githubusercontent.com/mkleinbort/Kaggle-COMPAS/main/train/y_train.csv', squeeze=True)\n",
    "X_test = pd.read_csv ('https://raw.githubusercontent.com/mkleinbort/Kaggle-COMPAS/main/test/X_test.csv')\n",
    "y_test = np.full(len(X_test), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([X_train, X_test])\n",
    "\n",
    "df = df.drop(['v_screening_date'], axis=1) # duplicate of 'screening_date'\n",
    "df = df.drop(['v_type_of_assessment'], axis=1) # duplicate of 'type_of_assessment'\n",
    "df = df.drop(['type_of_assessment'], axis=1) # 0 variance\n",
    "\n",
    "df['target'] = y_train.to_list() + list(y_test) # set target row-wise (ignoring indices)\n",
    "# df['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Engineer a new targets \n",
    "df['target_int'] = df['target'].replace({'No-Recidivism': 0, 'Non-Violent': 1, 'Violent': 2}) # so we can do regression\n",
    "df['target_bool'] = df['target'].replace({'No-Recidivism': 0, 'Non-Violent': 1, 'Violent': 1}) # 0 - no-rec, 1 - rec\n",
    "target_cols = ['target', 'target_int', 'target_bool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Engineer new date based features\n",
    "\n",
    "\"\"\"\n",
    "Custody: when someone is kept in prison until they go to court\n",
    "Screening:  involves  using  a  brief  instrument  to  quickly  capture basic information \n",
    "    about a personâ€™s risk to reoffend and is  used  to  determine  if  a  more  comprehensive  assessment\n",
    "    is  warranted.  In  a  jail  setting,  everyone,  regardless  of  legal  status, should be screened \n",
    "    at booking.  Risk  screening  divides  the  jail  population  into  high-,  medium-,  and  low-risk \n",
    "    categories,  making  it  possible  to  direct  intervention  resources  first  to  the highest-risk individuals.\n",
    "\n",
    "\n",
    "\n",
    "- c_arrest_date and c_offense_date are mutually exclusive and one of them is 99.8% likely to be filled\n",
    "- c_arrest_date and c_are mutually exclusive\n",
    "- start is number of days between c_jail_in and c_jail_out, maybe worth engineering total hours/minutes between\n",
    "- screening_date is always filled\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "days_cols = []\n",
    "\n",
    "def days_between(d1, d2):\n",
    "    try:\n",
    "        try:\n",
    "            d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            d1 = datetime.strptime(d1, \"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "        try:\n",
    "            d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            d2 = datetime.strptime(d2, \"%Y-%m-%d %H:%M:%S\")\n",
    "        return abs((d2 - d1).days)\n",
    "    except TypeError as e:\n",
    "        return None\n",
    "\n",
    "# mutually exclusive and one of them is always filled\n",
    "df['c_bad_date'] = df['c_arrest_date'].fillna(df['c_offense_date']).fillna(df['in_custody']).fillna(df['screening_date'])\n",
    "\n",
    "# how many days in custody\n",
    "df['custody_days'] = df.apply(lambda x: days_between(x.out_custody, x.in_custody), axis=1)\n",
    "\n",
    "# how quickly after going into custody did the person get screened (might indicate something?)\n",
    "df['in_custody_to_screening_days'] = df.apply(lambda x: days_between(x.in_custody, x.screening_date), axis=1)\n",
    "\n",
    "# questionable col because it is age related... Also corr=1 with age\n",
    "df['birth_to_bad_days'] = df.apply(lambda x: days_between(x.date_of_birth, x.c_bad_date), axis=1)\n",
    "\n",
    "# How many days passed until recitivism occcured. Golden feature!\n",
    "df['days_until_recitivism'] = df.apply(lambda x: days_between(x.c_jail_out, x.r_jail_in), axis=1)\n",
    "\n",
    "date_cols = [ \n",
    "    'c_arrest_date', \n",
    "    'c_offense_date',\n",
    "    'screening_date', \n",
    "    'in_custody', \n",
    "    'out_custody', \n",
    "    'date_of_birth',\n",
    "    'c_jail_in',\n",
    "    'c_jail_out',\n",
    "    'r_jail_in',\n",
    "    'r_jail_out',\n",
    "\n",
    "    # engineered\n",
    "    'c_bad_date'\n",
    "]\n",
    "\n",
    "duration_cols = [\n",
    "    'days_b_screening_arrest',\n",
    "    'custody_days',\n",
    "    'in_custody_to_screening_days',\n",
    "    'birth_to_bad_days',\n",
    "    'days_until_recitivism',\n",
    "]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All prior counts\n",
    "df['all_priors'] = df[[\n",
    "    'juv_fel_count',\n",
    "    'juv_misd_count', \n",
    "    'juv_other_count',\n",
    "    'priors_count',\n",
    "]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Has recidivism occured?\n",
    "df['has_r_jail_in'] = df['r_jail_in'].notna()\n",
    "df['has_r_jail_out'] = df['r_jail_out'].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values\n",
    "df['c_charge_desc'] = df['c_charge_desc'].fillna('missing') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "### Select features to use in modeling ###\n",
    "##########################################\n",
    "\n",
    "cols = [\n",
    "#     'name', useless\n",
    "#     'first', useless\n",
    "#     'last', useless\n",
    "#     'sex', # 0 importance + potential bias\n",
    "#     'date_of_birth', date\n",
    "#     'age', # innapropriate bias\n",
    "#     'age_group', # innapropriate bias\n",
    "#     'race', # innapropriate bias\n",
    "#     'juv_fel_count',\n",
    "#     'juv_misd_count', \n",
    "#     'juv_other_count',\n",
    "#     'priors_count', \n",
    "    'days_b_screening_arrest', # + AUC, - fairness\n",
    "#     'c_jail_in', date\n",
    "#     'c_jail_out',date\n",
    "#     'c_offense_date', date\n",
    "#     'c_arrest_date', date\n",
    "#     'c_charge_degree',  # 0 importance\n",
    "#     'c_charge_desc',\n",
    "#     'r_jail_in', date\n",
    "#     'r_jail_out', date\n",
    "#     'screening_date', date\n",
    "#     'in_custody', date\n",
    "#     'out_custody', date\n",
    "    'start', # + AUC, + fairness\n",
    "#     'target', target\n",
    "#     'target_int', target\n",
    "#     'target_bool', target\n",
    "#     'c_bad_date', date\n",
    "    'custody_days', # + AOC, + fairness\n",
    "    'in_custody_to_screening_days', # + AUC, + fairness\n",
    "#     'birth_to_bad_days', # corr=1 with age\n",
    "#     'has_r_jail_in', + AUC, - fairness\n",
    "#     'has_r_jail_out', # corr=1 with r_jail_in\n",
    "    'days_until_recitivism', # golden feature!\n",
    "    'all_priors', # the most racist/sexist/ageist feature, but without it it is even more unfair\n",
    "]\n",
    "\n",
    "cat_cols = [\n",
    "#     'sex',  # innapropriate bias\n",
    "#     'age_group', # innapropriate bias\n",
    "#     'race', # innapropriate bias\n",
    "#     'c_charge_degree', # 0 importance\n",
    "#     'c_charge_desc',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "df[cols].isna().sum()/len(df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric correlations across numeric features\n",
    "f, ax = plt.subplots(figsize=(16, 12))\n",
    "corr = df[cols].corr()\n",
    "mask = np.triu(corr)\n",
    "sns.heatmap(corr.select_dtypes('number'), annot=True, center=0, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['race'] = df['race'].replace({'Native American': 'Other'})\n",
    "# df['race'] = df['race'].replace({'Native American': 'Other', 'Asian': 'Other'})\n",
    "# df['race'] = df['race'].replace({'Native American': 'Other', 'Asian': 'Other', 'Hispanic': 'Other'})\n",
    "df['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age_group'] # we don't care about the actual age anyway\n",
    "df['race_age'] = df['race'] + df['age_group']\n",
    "df['race_sex'] = df['race'] + df['sex']\n",
    "df['sex_age'] = df['sex'] + df['age_group']\n",
    "df['race_sex_age'] = df['race'] + df['sex'] + df['age_group']\n",
    "df['target_race_sex_age'] = df['target'] + df['race'] + df['sex'] + df['age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df.race.unique()\n",
    "sexes = df.sex.unique()\n",
    "age_groups = df.age_group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparamets\n",
    "hyperparams = dict(\n",
    "    iterations=100,\n",
    "    learning_rate=1,\n",
    "    depth=5,\n",
    "    loss_function='MultiClass'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df.target.notna()]\n",
    "df_test = df[df.target.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))[1:]\n",
    "\n",
    "powerset(['race', 'sex', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_calm(y_true, y_score, *, average=\"macro\", sample_weight=None,\n",
    "                  max_fpr=None, multi_class=\"raise\", labels=None):\n",
    "    try:\n",
    "        return roc_auc_score(**locals())\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from numpy import where\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#define dataset\n",
    "X, y = make_classification(n_samples=100000,\n",
    "                           flip_y=0,\n",
    "                           random_state=1,\n",
    "                          )\n",
    "\n",
    "#summarises class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "#transforms dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "df = df.fillna(0)\n",
    "df['target_race_sex_age'] = pd.get_dummies(df[\"target_race_sex_age\"]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_oversampled, df_y_oversampled = oversample.fit_resample(df[cols].round(), df.target_race_sex_age)\n",
    "x_df = pd.DataFrame(df_x_oversampled, columns=cols)\n",
    "y_df = pd.DataFrame(df_y_oversampled)\n",
    "dfs_x_y = [x_df, y_df]\n",
    "\n",
    "df_merged = pd.concat(dfs_x_y)\n",
    "df_merged= df_merged.fillna(0)\n",
    "# df_merged.sex = df_merged.sex.astype('int')\n",
    "# df_merged.age = df_merged.age.astype('int')\n",
    "# df_merged.race = df_merged.race.astype('int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try predicting race/sex/age\n",
    "To see if we can, ideally we shouldn't\n",
    "Feature importances also will tell us the most racist/sexist/ageist features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import collections\n",
    "from fairlearn.metrics import MetricFrame\n",
    "\n",
    "def simple_cross_val(target_col, n_splits=5, n_repeats=5, verbose=False):\n",
    "    print(f'Target col: {target_col}')\n",
    "    kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "\n",
    "    roc_aucs = []\n",
    "\n",
    "    feature_importances = []\n",
    "    for split_idx, (train, test) in enumerate(kf.split(df_train[cols], df_train[target_col])): # Stratified on race\n",
    "        df_train_cv = df.iloc[train].reset_index(drop=True)\n",
    "        df_eval_cv = df.iloc[test].reset_index(drop=True)\n",
    "\n",
    "        train_dataset = Pool(data=df_train_cv[cols],\n",
    "                             label=df_train_cv[target_col],\n",
    "                             cat_features=cat_cols)\n",
    "\n",
    "        eval_dataset = Pool(data=df_eval_cv[cols],\n",
    "                            label=df_eval_cv[target_col],\n",
    "                            cat_features=cat_cols)\n",
    "\n",
    "        y_eval = pd.get_dummies(df_eval_cv[target_col]).to_numpy()\n",
    "        # Initialize CatBoostClassifier\n",
    "        model = CatBoostClassifier(**hyperparams)\n",
    "        # Fit model\n",
    "        model.fit(train_dataset, verbose=False)\n",
    "    #     # Get predicted probabilities for each class\n",
    "        preds_proba = model.predict_proba(eval_dataset)\n",
    "\n",
    "        roc_auc = roc_auc_score(y_eval, preds_proba, multi_class=\"ovo\")\n",
    "        roc_aucs.append(roc_auc)\n",
    "\n",
    "        feature_importances.append(model.get_feature_importance())\n",
    "        # avg_precs.append(avg_prec)\n",
    "        if verbose:\n",
    "            print(f'Split {split_idx+1:2}: ROC-AUC: {roc_auc*100:.2f}%. ')\n",
    "\n",
    "    # Evaluation\n",
    "\n",
    "    print(f'ROC-AUC {np.mean(roc_aucs)*100:.2f}% (STD: {np.std(roc_aucs)*100:.2f}%)')\n",
    "\n",
    "    # Feature importances\n",
    "    fi_df = pd.DataFrame({'feature': cols})\n",
    "    fi_df['importance'] = np.mean(feature_importances, axis=0)\n",
    "    fi_df['std'] = np.std(feature_importances, axis=0)\n",
    "    fi_df['min'] = np.min(feature_importances, axis=0)\n",
    "    fi_df['max'] = np.max(feature_importances, axis=0)\n",
    "    fi_df = fi_df.sort_values('importance', ascending=False)\n",
    "#     display(fi_df)\n",
    "    display(fi_df[['feature', 'importance']].sort_values('importance', ascending=True).plot.barh(x='feature'))\n",
    "    \n",
    "simple_cross_val('race')\n",
    "simple_cross_val('sex')\n",
    "simple_cross_val('age_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import collections\n",
    "from fairlearn.metrics import MetricFrame\n",
    "\n",
    "TARGET_COL = 'target'\n",
    "\n",
    "N_SPLITS = 5\n",
    "N_REPEATS = 10\n",
    "\n",
    "# Stratify based on intersections of race, age_group, sex and target.\n",
    "strat_df = pd.DataFrame(df_train[TARGET_COL].astype('str') + df_train['race_sex_age'], columns=['name'])\n",
    "\n",
    "# If cross section is too small for stratification to work (N < N_SPLITS) we mark it as an outlier\n",
    "strat_df.loc[strat_df.groupby('name').name.transform('count').lt(N_SPLITS), 'name'] = 'Outliers'    \n",
    "\n",
    "kf = RepeatedStratifiedKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=42)\n",
    "\n",
    "roc_aucs = []\n",
    "fairnesses = []\n",
    "\n",
    "\n",
    "feature_importances = []\n",
    "for split_idx, (train, test) in enumerate(kf.split(df_train[cols], strat_df['name'])): # Stratified on race\n",
    "    df_train_cv = df.iloc[train].reset_index(drop=True)\n",
    "    df_eval_cv = df.iloc[test].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = Pool(data=df_train_cv[cols],\n",
    "                         label=df_train_cv[TARGET_COL],\n",
    "                         cat_features=cat_cols)\n",
    "\n",
    "    eval_dataset = Pool(data=df_eval_cv[cols],\n",
    "                        label=df_eval_cv[TARGET_COL],\n",
    "                        cat_features=cat_cols)\n",
    "    \n",
    "    y_eval = pd.get_dummies(df_eval_cv[TARGET_COL]).to_numpy()\n",
    "    # Initialize CatBoostClassifier\n",
    "    model = CatBoostClassifier(**hyperparams)\n",
    "    # Fit model\n",
    "    model.fit(train_dataset, verbose=False)\n",
    "#     # Get predicted probabilities for each class\n",
    "    preds_proba = model.predict_proba(eval_dataset)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_eval, preds_proba, multi_class=\"ovo\")\n",
    "    roc_aucs.append(roc_auc)\n",
    "    \n",
    "    group_roc_aucs = []\n",
    "    for s in powerset(['race', 'sex', 'age']):\n",
    "        group_roc_aucs.append(MetricFrame(roc_auc_score_calm, y_eval, preds_proba, sensitive_features=df_eval_cv['_'.join(s)]).by_group)\n",
    "    fairness = 1 - np.sqrt(pd.concat(group_roc_aucs).std())\n",
    "    fairnesses.append(fairness)\n",
    "        \n",
    "    feature_importances.append(model.get_feature_importance())\n",
    "    # avg_precs.append(avg_prec)\n",
    "    print(f'Split {split_idx+1:2}: ROC-AUC: {roc_auc*100:.2f}%. '\n",
    "          f'Fairness: {fairness*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "print(f'ROC-AUC {np.mean(roc_aucs)*100:.2f}% (STD: {np.std(roc_aucs)*100:.2f}%)') # 90.48\n",
    "print(f'Fairness: {np.mean(fairnesses)*100:.2f}% (STD: {np.std(fairnesses)*100:.2f}%)') # 83.92\n",
    "\n",
    "score = 0.5 * np.mean(roc_aucs) + \\\n",
    "        0.5 * (np.mean(fairnesses))\n",
    "print(f'Score: {score*100:.2f}%') # 90.48\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import collections\n",
    "from fairlearn.metrics import MetricFrame\n",
    "\n",
    "TARGET_COL = 'target'\n",
    "\n",
    "train_dataset = Pool(data=df_train[cols],\n",
    "                     label=df_train[TARGET_COL],\n",
    "                     cat_features=cat_cols)\n",
    "\n",
    "eval_dataset = Pool(data=df_test[cols],\n",
    "                    label=None,\n",
    "                    cat_features=cat_cols)\n",
    "\n",
    "# y_eval = pd.get_dummies(eval_df[TARGET_COL]).to_numpy()\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(**hyperparams)\n",
    "# Fit model\n",
    "model.fit(train_dataset, verbose=True)\n",
    "# Get predicted classes\n",
    "preds_class = model.predict(eval_dataset)\n",
    "#     # Get predicted probabilities for each class\n",
    "preds_proba = model.predict_proba(eval_dataset)\n",
    "#     # Get predicted RawFormulaVal\n",
    "#     preds_raw = model.predict(eval_dataset, \n",
    "#                               prediction_type='RawFormulaVal')\n",
    "\n",
    "feature_importances.append(model.get_feature_importance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "fi_df = pd.DataFrame({'feature': cols})\n",
    "fi_df['importance'] = np.mean(feature_importances, axis=0)\n",
    "fi_df['std'] = np.std(feature_importances, axis=0)\n",
    "fi_df['min'] = np.min(feature_importances, axis=0)\n",
    "fi_df['max'] = np.max(feature_importances, axis=0)\n",
    "fi_df = fi_df.sort_values('importance', ascending=False)\n",
    "display(fi_df)\n",
    "display(fi_df[['feature', 'importance']].sort_values('importance', ascending=True).plot.barh(x='feature'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_cols = ['No-Recidivism', 'Non-Violent', 'Violent']\n",
    "out_df = pd.DataFrame(preds_proba, columns=out_cols)\n",
    "\n",
    "# Make sure that classes are in the right order and the highest prediction corresponds to predicted class\n",
    "out_df['class'] = out_df.apply(lambda x: out_cols[np.argmax(x[out_cols])], axis=1)\n",
    "assert (out_df['class'] == preds_class.ravel()).all()\n",
    "out_df = out_df[out_cols]\n",
    "\n",
    "out_df.to_csv('y_test.csv', index=False)\n",
    "\n",
    "import urllib.parse\n",
    "upload_link = urllib.parse.quote('share.streamlit.io/mkleinbort/kaggle-compas/main/app.py')\n",
    "y_test_link = urllib.parse.quote('y_test.csv')\n",
    "print(f'Predictions were exported to \"y_test.csv\". Upload them at https://{upload_link}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations log\n",
    "## 30/03/2021 22:18 JK\n",
    "### Columns\n",
    "['days_b_screening_arrest',\n",
    " 'start',\n",
    " 'custody_days',\n",
    " 'in_custody_to_screening_days',\n",
    " 'days_until_recitivism',\n",
    " 'all_priors']\n",
    " \n",
    "### Hyperparams\n",
    "```\n",
    "iterations=100,\n",
    "learning_rate=1, (BEST)\n",
    "depth=3, (BEST)\n",
    "loss_function='MultiClass'\n",
    "```\n",
    "\n",
    "## Results\n",
    "### CV\n",
    "```\n",
    "ROC-AUC 88.65% (STD: 1.68%)\n",
    "Fairness: 73.54% (STD: 3.81%)\n",
    "Score: 81.10%\n",
    "```\n",
    "\n",
    "### Online\n",
    "```\n",
    "You scored: 87.20% in accuracy and 68.96% in fairness.\n",
    "\n",
    "Overall Score: 78.08%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baysian Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising using Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from skopt import gp_minimize\n",
    "\n",
    "# defining the kernel for the Gaussian process\n",
    "kernel = Matern(length_scale=1.0)\n",
    "TARGET_COL = 'target_int'\n",
    "\n",
    "X = df_train[cols].fillna(0)\n",
    "y = df_train[TARGET_COL]\n",
    "\n",
    "# initialise number of queries\n",
    "N_QUERY = 5\n",
    "\n",
    "# initialise the regressor\n",
    "gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)\n",
    "gpr.score(X, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "\n",
    "res = gp_minimize()\n",
    "plot_convergence(res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# with plt.style.context('seaborn-white'):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.scatter(optimizer.X_training, optimizer.y_training, c='k', s=50, label='Queried')\n",
    "#     plt.scatter(X_max, y_max, s=100, c='r', label='Current optimum')\n",
    "#     plt.plot(X.ravel(), y, c='k', linewidth=2, label='Function')\n",
    "#     plt.plot(X.ravel(), y_pred, label='GP regressor')\n",
    "#     plt.fill_between(X.ravel(), y_pred - y_std, y_pred + y_std, alpha=0.5)\n",
    "#     plt.title('First five queries of Bayesian optimization')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
